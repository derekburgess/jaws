{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import asyncio\n",
    "from typing import Annotated, AsyncGenerator\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread, GroupChatOrchestration, RoundRobinGroupChatManager\n",
    "from semantic_kernel.agents.runtime import InProcessRuntime\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings, OpenAIChatCompletion\n",
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from semantic_kernel.contents import FunctionCallContent, FunctionResultContent\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from jaws.jaws_config import *\n",
    "from jaws.jaws_utils import dbms_connection\n",
    "\n",
    "# Database not passed, uses the database set in jaws_config.py\n",
    "driver = dbms_connection(DATABASE)\n",
    "kernel = Kernel()\n",
    "settings = OpenAIChatPromptExecutionSettings()\n",
    "reasoning_service = OpenAIChatCompletion(ai_model_id=OPENAI_REASONING_MODEL, api_key=OPENAI_API_KEY)\n",
    "kernel.add_service(reasoning_service)\n",
    "lang_service = OpenAIChatCompletion(ai_model_id=OPENAI_MODEL, api_key=OPENAI_API_KEY)\n",
    "kernel.add_service(lang_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Kernel Helper Functions\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    for item in message.items or []:\n",
    "        if isinstance(item, FunctionCallContent):\n",
    "            print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n",
    "        elif isinstance(item, FunctionResultContent):\n",
    "            print(f\"Function Result:> {item.result} for function: {item.name}\")\n",
    "        else:\n",
    "            print(f\"{message.role}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "class ListInterfaces:\n",
    "    @kernel_function(description=\"List available network interfaces. You will never want to select interfaces such as; 'lo', 'docker0', 'wlo1', etc.\")\n",
    "    def list_interfaces(self) -> Annotated[str, \"A list of available network interfaces.\"]:\n",
    "        interfaces = subprocess.run(['python', './jaws/jaws_capture.py', '--list', '--agent'], capture_output=True, text=True)\n",
    "        return str(interfaces.stdout)\n",
    "\n",
    "\n",
    "class CapturePackets:\n",
    "    @kernel_function(description=\"Captures packets into the database. Choose a duration depending on the amount of data you want to capture. Recommended not to exceed 60 seconds.\")\n",
    "    def capture_packets(self, interface: str, duration: int) -> Annotated[str, \"A process status message once the process is complete.\"]:\n",
    "        if duration > 60:\n",
    "            duration = 60\n",
    "        packets = subprocess.run(['python', './jaws/jaws_capture.py', '--interface', interface, '--duration', str(duration), '--agent'], capture_output=True, text=True)\n",
    "        return str(packets.stdout)\n",
    "\n",
    "\n",
    "class DocumentOrganizations:\n",
    "    @kernel_function(description=\"Enriches data with organizations by looking up IP addresses.\")\n",
    "    def document_organizations(self) -> Annotated[str, \"A process status message once the process is complete.\"]:\n",
    "        organizations = subprocess.run(['python', './jaws/jaws_ipinfo.py', '--agent'], capture_output=True, text=True)\n",
    "        return str(organizations.stdout)\n",
    "\n",
    "\n",
    "class ComputeEmbeddings:\n",
    "    @kernel_function(description=\"Transforms the network traffic data into embeddings for analysis.\")\n",
    "    def compute_embeddings(self) -> Annotated[str, \"A process status message once the process is complete.\"]:\n",
    "        embeddings = subprocess.run(['python', './jaws/jaws_compute.py', '--agent'], capture_output=True, text=True)\n",
    "        return str(embeddings.stdout)\n",
    "    \n",
    "\n",
    "class AnomalyDetection:\n",
    "    @kernel_function(description=\"Transforms the network traffic data and embeddings into a list of anomalies.\")\n",
    "    def anomoly_detection(self) -> Annotated[str, \"A string containing a list of anomalies.\"]:\n",
    "        output = subprocess.run(['python', './jaws/jaws_finder.py', '--agent'], capture_output=True, text=True)\n",
    "        return str(output.stdout)\n",
    "    \n",
    "\n",
    "class FetchTraffic:\n",
    "    @kernel_function(description=\"Step 2: Fetches the latest traffic data from the database and returns it as a string.\")\n",
    "    def fetch_traffic(self) -> Annotated[str, \"A string containing a list of current traffic data.\"]:\n",
    "        query = \"\"\"\n",
    "        MATCH (traffic:TRAFFIC)\n",
    "        WHERE traffic.TIMESTAMP > datetime() - duration({minutes: 10})\n",
    "        RETURN DISTINCT\n",
    "            traffic.IP_ADDRESS AS ip_address,\n",
    "            traffic.PORT AS port,\n",
    "            traffic.ORGANIZATION AS org,\n",
    "            traffic.HOSTNAME AS hostname,\n",
    "            traffic.LOCATION AS location,\n",
    "            traffic.TOTAL_SIZE AS total_size,\n",
    "            traffic.OUTLIER AS outlier,\n",
    "            traffic.TIMESTAMP AS timestamp\n",
    "        ORDER BY traffic.TIMESTAMP DESC\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "        with driver.session(database=DATABASE) as session:\n",
    "            result = session.run(query)\n",
    "            data = []\n",
    "            for record in result:\n",
    "                data.append({\n",
    "                    'ip_address': record['ip_address'],\n",
    "                    'port': record['port'],\n",
    "                    'org': record['org'],\n",
    "                    'hostname': record['hostname'],\n",
    "                    'location': record['location'],\n",
    "                    'total_size': record['total_size'],\n",
    "                    'outlier': record['outlier'],\n",
    "                    'timestamp': record['timestamp']\n",
    "                })\n",
    "            return str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "ANALYST_PROMPT = \"\"\"You are an expert IT Professional, Sysadmin, and Analyst. Your task is to capture network packets and perform ETL(Extract, Transform, and Load) on the network data to prepare it for analysis. Once the network traffic data is prepared, you task is to analyze it for anomalies and patterns. You have access to several tools to accomplish this, but the process is faily linear, and looks something like this:\n",
    "\n",
    "1. Use tool: ListInterfaces() to list and select an interface. You will never want to select interfaces such as; 'lo', 'docker0', 'wlo1', etc.\n",
    "2. Use tool: CapturePackets() to capture network traffic. This is a critical step, as packet data is the foundation of the analysis.\n",
    "3. Use tool: DocumentOrganizations() to document organizations using captured ip address data. This is an important step, as it enriches the packet data with organization ownership information.\n",
    "4. Use tool: ComputeEmbeddings() to compute embeddings from traffic data. This is an important step, as it transforms the data into traffic embeddings.\n",
    "5. Use tool: AnomalyDetection() to analyze the traffic data for anomalies and patterns.\n",
    "6. Use tool: FetchTraffic() to fetch the final enriched and transformed network traffic data from the database to be used for analysis and reporting.\n",
    "7. Return a report of your findings using the following format:\n",
    "\n",
    "Executive Summary:\n",
    "A concise summary of the traffic analysis, including a description of the cluster plot.\n",
    "\n",
    "Traffic Patterns: \n",
    "Identify and describe the regular traffic patterns. Highlight any anomalies or unusual patterns.\n",
    "\n",
    "Recommendations:\n",
    "1. Recommendations: List detailed recommendations for enhancing security based on the traffic patterns identified.\n",
    "2. Rationale: Provide a rationale for each recommendation, explaining how it addresses specific issues identified in the traffic analysis.\n",
    "\"\"\"\n",
    "\n",
    "ANALYST_MANAGED_PROMPT = \"\"\"You are an expert IT Professional, Sysadmin, and Analyst. Your task is to capture network packets and perform ETL, Extract, Transform, and Load using the network data to prepare it for downstream analysis. You have access to several tools, but the process is faily linear. and looks something like this:\n",
    "\n",
    "1. Use tool: ListInterfaces() to list and select an interface. You will never want to select interfaces such as; 'lo', 'docker0', 'wlo1', etc.\n",
    "2. Use tool: CapturePackets() to capture network traffic. This is a critical step, as packet data is the foundation of the analysis.\n",
    "3. Use tool: DocumentOrganizations() to document organizations using captured ip address data. This is an important step, as it enriches the packet data with organization ownership information.\n",
    "4. Use tool: ComputeEmbeddings() to compute embeddings from traffic data. This is an important step, as it transforms the data into traffic embeddings.\n",
    "\"\"\"\n",
    "\n",
    "MANAGER_PROMPT = \"\"\"You are an expert IT Professional, Sysadmin, and Analyst. Your task is to review data from network traffic to identify patterns and make recommendations for security configurations. \n",
    "\n",
    "You can use the FetchTraffic() tool to check if there is any data available. If data exists, you can use the anomoly_detection tool to detect anomalies.\n",
    "\n",
    "If there is no data, or an empty DataFrame is returned, you should leverage the network_analyst agent you manage to capture and process network traffic. It is very expensive\n",
    "to collect and store network traffic data, so do not recommend that the network_analyst agent collect more than 60 seconds of data.\n",
    "\n",
    "Since data is being collected over short periods of time. You should always consider collecting fresh data before peforming your analysis. It is recommended that you consider running \n",
    "fetch_data to see what data is available, but not not limit yourself to these outputs as they may be outdated, and consider requesting fresh data from the network_analyst agent.\n",
    "\n",
    "When you have access to fresh data, return a brief report in the following format:\n",
    "\n",
    "Executive Summary:\n",
    "A concise summary of the traffic analysis, including a description of the cluster plot.\n",
    "\n",
    "Traffic Patterns: \n",
    "Identify and describe the regular traffic patterns. Highlight any anomalies or unusual patterns.\n",
    "\n",
    "Recommendations:\n",
    "1. Recommendations: List detailed recommendations for enhancing security based on the traffic patterns identified.\n",
    "2. Rationale: Provide a rationale for each recommendation, explaining how it addresses specific issues identified in the traffic analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents\n",
    "network_analyst = ChatCompletionAgent(\n",
    "    service=lang_service, \n",
    "    name=\"NetworkAnalyst\",\n",
    "    description=\"An expert IT Professional, Sysadmin, and Analyst. Tasked with capturing network packets and perform ETL(Extract, Transform, and Load) on the network data to prepare it for analysis.\", # Once the network traffic data is prepared, tasked with analyzing the data for anomalies and patterns.\n",
    "    instructions=ANALYST_MANAGED_PROMPT,\n",
    "    plugins=[ListInterfaces(), CapturePackets(), DocumentOrganizations(), ComputeEmbeddings()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")\n",
    "\n",
    "# For managed orchestration.\n",
    "lead_network_analyst = ChatCompletionAgent(\n",
    "    service=reasoning_service,\n",
    "    name=\"LeadAnalyst\",\n",
    "    description=\"An expert IT Professional, Sysadmin, and Analyst. Tasked with reviewing network traffic data to identify patterns, anomalies, and make recommendations for security configurations.\",\n",
    "    instructions=MANAGER_PROMPT,\n",
    "    plugins=[FetchTraffic(), AnomalyDetection()],\n",
    "    arguments=KernelArguments(settings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Chat Orchestration\n",
    "members=[network_analyst, lead_network_analyst]\n",
    "max_rounds = 2\n",
    "\n",
    "\n",
    "def response_callback(message: ChatMessageContent) -> None:\n",
    "    print(f\"**{message.name}**\\n{message.content}\")\n",
    "\n",
    "\n",
    "async def group_chat(input: str) -> str:\n",
    "    group_chat_orchestration = GroupChatOrchestration(\n",
    "        members=members,\n",
    "        manager=RoundRobinGroupChatManager(max_rounds=max_rounds),\n",
    "        agent_response_callback=response_callback,\n",
    "    )\n",
    "\n",
    "    runtime = InProcessRuntime()\n",
    "    runtime.start()\n",
    "\n",
    "    print(f\"[INPUT] | {input}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"[ORCHESTRATION] | [GROUP CHAT] {len(members)} | [ROUND ROBIN] {max_rounds}\")\n",
    "        orchestration_result = await group_chat_orchestration.invoke(\n",
    "            task=input,\n",
    "            runtime=runtime\n",
    "        )\n",
    "        \n",
    "        response = await orchestration_result.get()\n",
    "        \n",
    "        if hasattr(response, 'content'):\n",
    "            response_text = response.content\n",
    "        elif hasattr(response, 'inner_content') and hasattr(response.inner_content, 'content'):\n",
    "            response_text = response.inner_content.content\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "        \n",
    "        print(f\"[RESPONSE] | {response_text}\")\n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] | {str(e)}\")\n",
    "        return \"\"\n",
    "        \n",
    "    finally:\n",
    "        await runtime.stop_when_idle()\n",
    "\n",
    "\n",
    "async def request_report():\n",
    "    response = await group_chat(\"The command center is reporting suspicious activity at your endpoint, please probe the network traffic and report back ASAP.\")\n",
    "    formatted_response = {\"role\": \"assistant\", \"content\": response, \"metadata\": {\"title\": \"ðŸ”® Situation Report\"}}\n",
    "    return [formatted_response]\n",
    "\n",
    "#response = await request_report()\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface\n",
    "with gr.Blocks(title=\"Network Traffic Analysis\") as INTERFACE:\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            CHATBOT = gr.Chatbot(\n",
    "                value=[{\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": f\"[ORCHESTRATION] | [GROUP CHAT] {len(members)} | [ROUND ROBIN] {max_rounds}\\nClick 'Request Report' to start the network analysis.\\nThe process consists of the following activities:\\n1. List and select an interface.\\n2. Capture network traffic on the selected interface.\\n3. Document organizations using captured ip address data.\\n4. Compute embeddings from traffic data.\\n5. Analyze the traffic data for anomalies and patterns.\\n6. Return a situation report.\",\n",
    "                    \"metadata\": {\"title\": \"ðŸª¬ Network Traffic Analysis\"}\n",
    "                }],\n",
    "                type=\"messages\",\n",
    "                show_label=False,\n",
    "                autoscroll=True,\n",
    "                resizable=True,\n",
    "                show_copy_button=True,\n",
    "                height=320\n",
    "            )\n",
    "        with gr.Row():\n",
    "            ANALYZE_BUTTON = gr.Button(\n",
    "                \"ðŸ”® Request Report\"\n",
    "            )\n",
    "\n",
    "    ANALYZE_BUTTON.click(\n",
    "        fn=request_report,\n",
    "        outputs=[CHATBOT]\n",
    "    )\n",
    "    \n",
    "INTERFACE.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Off Orchestration | NOT IN USE\n",
    "async def hand_off() -> None:\n",
    "    thread: ChatHistoryAgentThread = None\n",
    "    print(\"[READY] | [ExpertAdvisor]\")\n",
    "    while True:\n",
    "        user_input = input(\"[INPUT] \")\n",
    "\n",
    "        if user_input.lower().strip() == \"that is all\":\n",
    "            print(\"\\n\\n[EXITING]\")\n",
    "            return False\n",
    "\n",
    "        print(f\"[ORCHESTRATION] | [HAND OFF] [THREAD]\")\n",
    "        async for response in lead_network_analyst.invoke(\n",
    "            messages=user_input,\n",
    "            thread=thread,\n",
    "            on_intermediate_message=handle_intermediate_steps,\n",
    "        ):\n",
    "            print(f\"# {response.role}: {response}\")\n",
    "            thread = response.thread"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
